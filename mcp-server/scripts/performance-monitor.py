#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCPæ–‡æ¡£æœåŠ¡å™¨æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡ç³»ç»Ÿ
ç›‘æ§æ–‡æ¡£ä½¿ç”¨æƒ…å†µã€AIæŸ¥è¯¢æ€§èƒ½å’Œæ›´æ–°é¢‘ç‡
"""

import os
import json
import sqlite3
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import argparse
from dataclasses import dataclass, asdict
from collections import defaultdict
import threading

logger = logging.getLogger(__name__)

@dataclass
class AccessEvent:
    """æ–‡æ¡£è®¿é—®äº‹ä»¶"""
    timestamp: str
    document_path: str
    access_type: str  # read, write, create, delete
    user_agent: str
    duration_ms: Optional[int] = None
    success: bool = True
    error_message: Optional[str] = None

@dataclass
class AIQueryEvent:
    """AIæŸ¥è¯¢äº‹ä»¶"""
    timestamp: str
    query_type: str  # template_generation, code_analysis, quality_check
    project_path: str
    language: str
    duration_ms: int
    tokens_processed: Optional[int] = None
    success: bool = True
    error_message: Optional[str] = None

@dataclass
class UpdateEvent:
    """æ–‡æ¡£æ›´æ–°äº‹ä»¶"""
    timestamp: str
    update_type: str  # automatic, manual, git_hook
    files_changed: int
    trigger: str  # code_change, scheduled, user_action
    success: bool = True
    duration_ms: Optional[int] = None

class MetricsDatabase:
    """æŒ‡æ ‡æ•°æ®åº“"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_database()
        self._lock = threading.Lock()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡æ¡£è®¿é—®è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_access (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    document_path TEXT NOT NULL,
                    access_type TEXT NOT NULL,
                    user_agent TEXT,
                    duration_ms INTEGER,
                    success BOOLEAN NOT NULL,
                    error_message TEXT
                )
            """)
            
            # AIæŸ¥è¯¢è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_queries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    query_type TEXT NOT NULL,
                    project_path TEXT NOT NULL,
                    language TEXT NOT NULL,
                    duration_ms INTEGER NOT NULL,
                    tokens_processed INTEGER,
                    success BOOLEAN NOT NULL,
                    error_message TEXT
                )
            """)
            
            # æ–‡æ¡£æ›´æ–°è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_updates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    update_type TEXT NOT NULL,
                    files_changed INTEGER NOT NULL,
                    trigger TEXT NOT NULL,
                    success BOOLEAN NOT NULL,
                    duration_ms INTEGER
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_access_timestamp ON document_access(timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_access_path ON document_access(document_path)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_queries_timestamp ON ai_queries(timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_queries_type ON ai_queries(query_type)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_updates_timestamp ON document_updates(timestamp)")
            
            conn.commit()
    
    def record_access(self, event: AccessEvent):
        """è®°å½•æ–‡æ¡£è®¿é—®äº‹ä»¶"""
        with self._lock:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO document_access 
                    (timestamp, document_path, access_type, user_agent, duration_ms, success, error_message)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    event.timestamp, event.document_path, event.access_type, 
                    event.user_agent, event.duration_ms, event.success, event.error_message
                ))
                conn.commit()
    
    def record_ai_query(self, event: AIQueryEvent):
        """è®°å½•AIæŸ¥è¯¢äº‹ä»¶"""
        with self._lock:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO ai_queries 
                    (timestamp, query_type, project_path, language, duration_ms, tokens_processed, success, error_message)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    event.timestamp, event.query_type, event.project_path, 
                    event.language, event.duration_ms, event.tokens_processed, event.success, event.error_message
                ))
                conn.commit()
    
    def record_update(self, event: UpdateEvent):
        """è®°å½•æ–‡æ¡£æ›´æ–°äº‹ä»¶"""
        with self._lock:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO document_updates 
                    (timestamp, update_type, files_changed, trigger, success, duration_ms)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    event.timestamp, event.update_type, event.files_changed, 
                    event.trigger, event.success, event.duration_ms
                ))
                conn.commit()
    
    def get_stats(self, start_date: str = None, end_date: str = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ„å»ºæ—¶é—´è¿‡æ»¤æ¡ä»¶
            time_filter = ""
            params = []
            if start_date:
                time_filter += " AND timestamp >= ?"
                params.append(start_date)
            if end_date:
                time_filter += " AND timestamp <= ?"
                params.append(end_date)
            
            stats = {}
            
            # æ–‡æ¡£è®¿é—®ç»Ÿè®¡
            cursor.execute(f"""
                SELECT 
                    COUNT(*) as total_access,
                    COUNT(CASE WHEN success THEN 1 END) as successful_access,
                    AVG(duration_ms) as avg_duration,
                    access_type,
                    COUNT(*) as count
                FROM document_access 
                WHERE 1=1 {time_filter}
                GROUP BY access_type
            """, params)
            
            access_stats = cursor.fetchall()
            stats["document_access"] = {
                "by_type": {row[3]: {"count": row[4], "success_rate": row[1]/row[0] if row[0] > 0 else 0} 
                           for row in access_stats},
                "total_requests": sum(row[0] for row in access_stats),
                "avg_duration_ms": sum(row[2] for row in access_stats if row[2]) / len([r for r in access_stats if r[2]]) if access_stats else 0
            }
            
            # AIæŸ¥è¯¢ç»Ÿè®¡
            cursor.execute(f"""
                SELECT 
                    query_type,
                    COUNT(*) as count,
                    AVG(duration_ms) as avg_duration,
                    AVG(tokens_processed) as avg_tokens,
                    COUNT(CASE WHEN success THEN 1 END) as successful
                FROM ai_queries 
                WHERE 1=1 {time_filter}
                GROUP BY query_type
            """, params)
            
            ai_stats = cursor.fetchall()
            stats["ai_queries"] = {
                "by_type": {
                    row[0]: {
                        "count": row[1],
                        "avg_duration_ms": row[2],
                        "avg_tokens": row[3],
                        "success_rate": row[4] / row[1] if row[1] > 0 else 0
                    } for row in ai_stats
                },
                "total_queries": sum(row[1] for row in ai_stats)
            }
            
            # æ–‡æ¡£æ›´æ–°ç»Ÿè®¡
            cursor.execute(f"""
                SELECT 
                    update_type,
                    trigger,
                    COUNT(*) as count,
                    SUM(files_changed) as total_files,
                    AVG(duration_ms) as avg_duration
                FROM document_updates 
                WHERE 1=1 {time_filter}
                GROUP BY update_type, trigger
            """, params)
            
            update_stats = cursor.fetchall()
            stats["document_updates"] = {
                "by_type_and_trigger": {
                    f"{row[0]}_{row[1]}": {
                        "count": row[2],
                        "total_files": row[3],
                        "avg_duration_ms": row[4]
                    } for row in update_stats
                },
                "total_updates": sum(row[2] for row in update_stats)
            }
            
            return stats

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, mcp_root: str, db_path: str = None):
        self.mcp_root = Path(mcp_root)
        self.db_path = db_path or str(self.mcp_root / "metrics.db")
        self.metrics_db = MetricsDatabase(self.db_path)
        self.start_time = datetime.now()
        
    def track_document_access(self, document_path: str, access_type: str, 
                            user_agent: str = "MCP-System", duration_ms: int = None):
        """è·Ÿè¸ªæ–‡æ¡£è®¿é—®"""
        event = AccessEvent(
            timestamp=datetime.now().isoformat(),
            document_path=document_path,
            access_type=access_type,
            user_agent=user_agent,
            duration_ms=duration_ms
        )
        self.metrics_db.record_access(event)
        logger.debug(f"Tracked document access: {document_path} ({access_type})")
    
    def track_ai_query(self, query_type: str, project_path: str, language: str, 
                      duration_ms: int, tokens_processed: int = None, success: bool = True):
        """è·Ÿè¸ªAIæŸ¥è¯¢"""
        event = AIQueryEvent(
            timestamp=datetime.now().isoformat(),
            query_type=query_type,
            project_path=project_path,
            language=language,
            duration_ms=duration_ms,
            tokens_processed=tokens_processed,
            success=success
        )
        self.metrics_db.record_ai_query(event)
        logger.debug(f"Tracked AI query: {query_type} for {project_path}")
    
    def track_document_update(self, update_type: str, files_changed: int, 
                            trigger: str, duration_ms: int = None, success: bool = True):
        """è·Ÿè¸ªæ–‡æ¡£æ›´æ–°"""
        event = UpdateEvent(
            timestamp=datetime.now().isoformat(),
            update_type=update_type,
            files_changed=files_changed,
            trigger=trigger,
            success=success,
            duration_ms=duration_ms
        )
        self.metrics_db.record_update(event)
        logger.info(f"Tracked document update: {update_type} ({files_changed} files)")
    
    def analyze_performance(self, days: int = 7) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ•°æ®"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        stats = self.metrics_db.get_stats(
            start_date.isoformat(), 
            end_date.isoformat()
        )
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        analysis = {
            "period": f"{days} days",
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "summary": {},
            "recommendations": []
        }
        
        # æ–‡æ¡£è®¿é—®åˆ†æ
        doc_access = stats.get("document_access", {})
        total_requests = doc_access.get("total_requests", 0)
        avg_duration = doc_access.get("avg_duration_ms", 0)
        
        analysis["summary"]["document_access"] = {
            "total_requests": total_requests,
            "requests_per_day": total_requests / days,
            "avg_response_time_ms": avg_duration
        }
        
        if avg_duration > 1000:  # è¶…è¿‡1ç§’
            analysis["recommendations"].append("Document access response time is slow (>1s). Consider optimizing file I/O.")
        
        # AIæŸ¥è¯¢åˆ†æ
        ai_queries = stats.get("ai_queries", {})
        total_queries = ai_queries.get("total_queries", 0)
        
        analysis["summary"]["ai_queries"] = {
            "total_queries": total_queries,
            "queries_per_day": total_queries / days
        }
        
        by_type = ai_queries.get("by_type", {})
        slowest_query_type = max(by_type.keys(), 
                               key=lambda x: by_type[x].get("avg_duration_ms", 0),
                               default=None)
        
        if slowest_query_type:
            slowest_duration = by_type[slowest_query_type]["avg_duration_ms"]
            analysis["summary"]["slowest_query_type"] = {
                "type": slowest_query_type,
                "avg_duration_ms": slowest_duration
            }
            
            if slowest_duration > 5000:  # è¶…è¿‡5ç§’
                analysis["recommendations"].append(f"AI query type '{slowest_query_type}' is slow (>{slowest_duration:.0f}ms). Consider optimization.")
        
        # æ–‡æ¡£æ›´æ–°åˆ†æ
        doc_updates = stats.get("document_updates", {})
        total_updates = doc_updates.get("total_updates", 0)
        
        analysis["summary"]["document_updates"] = {
            "total_updates": total_updates,
            "updates_per_day": total_updates / days
        }
        
        # ç”Ÿæˆå¥åº·è¯„åˆ†
        health_score = self._calculate_health_score(stats, days)
        analysis["health_score"] = health_score
        
        return analysis
    
    def _calculate_health_score(self, stats: Dict[str, Any], days: int) -> Dict[str, Any]:
        """è®¡ç®—ç³»ç»Ÿå¥åº·è¯„åˆ†"""
        score = 100
        factors = []
        
        # æ–‡æ¡£è®¿é—®æˆåŠŸç‡
        doc_access = stats.get("document_access", {})
        by_type = doc_access.get("by_type", {})
        if by_type:
            avg_success_rate = sum(t.get("success_rate", 1) for t in by_type.values()) / len(by_type)
            if avg_success_rate < 0.95:
                penalty = (0.95 - avg_success_rate) * 30
                score -= penalty
                factors.append(f"Document access success rate: {avg_success_rate:.1%} (-{penalty:.1f})")
        
        # AIæŸ¥è¯¢æˆåŠŸç‡
        ai_queries = stats.get("ai_queries", {})
        ai_by_type = ai_queries.get("by_type", {})
        if ai_by_type:
            avg_ai_success_rate = sum(t.get("success_rate", 1) for t in ai_by_type.values()) / len(ai_by_type)
            if avg_ai_success_rate < 0.90:
                penalty = (0.90 - avg_ai_success_rate) * 40
                score -= penalty
                factors.append(f"AI query success rate: {avg_ai_success_rate:.1%} (-{penalty:.1f})")
        
        # å“åº”æ—¶é—´
        avg_duration = doc_access.get("avg_duration_ms", 0)
        if avg_duration > 500:
            penalty = min((avg_duration - 500) / 100, 20)
            score -= penalty
            factors.append(f"Slow response time: {avg_duration:.0f}ms (-{penalty:.1f})")
        
        # æ›´æ–°é¢‘ç‡ï¼ˆè¿‡äºé¢‘ç¹å¯èƒ½è¡¨ç¤ºé—®é¢˜ï¼‰
        doc_updates = stats.get("document_updates", {})
        updates_per_day = doc_updates.get("total_updates", 0) / days
        if updates_per_day > 20:  # æ¯å¤©è¶…è¿‡20æ¬¡æ›´æ–°å¯èƒ½å¼‚å¸¸
            penalty = min((updates_per_day - 20) * 2, 15)
            score -= penalty
            factors.append(f"High update frequency: {updates_per_day:.1f}/day (-{penalty:.1f})")
        
        score = max(0, min(100, score))
        
        # è¯„çº§
        if score >= 90:
            grade = "A"
            status = "Excellent"
        elif score >= 80:
            grade = "B" 
            status = "Good"
        elif score >= 70:
            grade = "C"
            status = "Fair"
        elif score >= 60:
            grade = "D"
            status = "Poor"
        else:
            grade = "F"
            status = "Critical"
        
        return {
            "score": round(score, 1),
            "grade": grade,
            "status": status,
            "factors": factors
        }
    
    def generate_report(self, days: int = 7, output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        analysis = self.analyze_performance(days)
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "mcp_root": str(self.mcp_root),
            "analysis_period": f"{days} days",
            **analysis
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"Performance report saved to: {output_file}")
        
        return report
    
    def cleanup_old_metrics(self, days_to_keep: int = 30):
        """æ¸…ç†æ—§çš„æŒ‡æ ‡æ•°æ®"""
        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ¸…ç†å„è¡¨ä¸­çš„æ—§æ•°æ®
            tables = ["document_access", "ai_queries", "document_updates"]
            total_deleted = 0
            
            for table in tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE timestamp < ?", (cutoff_date,))
                count_before = cursor.fetchone()[0]
                
                cursor.execute(f"DELETE FROM {table} WHERE timestamp < ?", (cutoff_date,))
                deleted = cursor.rowcount
                total_deleted += deleted
                
                logger.info(f"Cleaned {deleted} old records from {table}")
            
            conn.commit()
            
            # å‹ç¼©æ•°æ®åº“
            cursor.execute("VACUUM")
            
            logger.info(f"Cleanup completed: removed {total_deleted} old records")

def main():
    parser = argparse.ArgumentParser(description="MCP Performance Monitor")
    parser.add_argument("--mcp-root", default=".", help="MCP root directory")
    parser.add_argument("--db-path", help="Database file path")
    parser.add_argument("--action", choices=["analyze", "report", "cleanup"], 
                       default="analyze", help="Action to perform")
    parser.add_argument("--days", type=int, default=7, help="Number of days to analyze")
    parser.add_argument("--output", help="Output file for reports")
    parser.add_argument("--cleanup-days", type=int, default=30, 
                       help="Days of data to keep during cleanup")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor(args.mcp_root, args.db_path)
    
    if args.action == "analyze":
        analysis = monitor.analyze_performance(args.days)
        
        print(f"=== MCPæ€§èƒ½åˆ†ææŠ¥å‘Š ({args.days}å¤©) ===")
        print(f"åˆ†ææœŸé—´: {analysis['start_date'][:10]} åˆ° {analysis['end_date'][:10]}")
        
        # å¥åº·è¯„åˆ†
        health = analysis["health_score"]
        print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·è¯„åˆ†: {health['score']}/100 ({health['grade']} - {health['status']})")
        
        if health["factors"]:
            print("å½±å“å› ç´ :")
            for factor in health["factors"]:
                print(f"  â€¢ {factor}")
        
        # æ‘˜è¦ç»Ÿè®¡
        summary = analysis["summary"]
        
        if "document_access" in summary:
            doc_stats = summary["document_access"]
            print(f"\nğŸ“„ æ–‡æ¡£è®¿é—®:")
            print(f"  â€¢ æ€»è¯·æ±‚æ•°: {doc_stats['total_requests']}")
            print(f"  â€¢ æ—¥å‡è¯·æ±‚: {doc_stats['requests_per_day']:.1f}")
            print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´: {doc_stats['avg_response_time_ms']:.0f}ms")
        
        if "ai_queries" in summary:
            ai_stats = summary["ai_queries"]
            print(f"\nğŸ¤– AIæŸ¥è¯¢:")
            print(f"  â€¢ æ€»æŸ¥è¯¢æ•°: {ai_stats['total_queries']}")
            print(f"  â€¢ æ—¥å‡æŸ¥è¯¢: {ai_stats['queries_per_day']:.1f}")
            
            if "slowest_query_type" in summary:
                slowest = summary["slowest_query_type"]
                print(f"  â€¢ æœ€æ…¢æŸ¥è¯¢ç±»å‹: {slowest['type']} ({slowest['avg_duration_ms']:.0f}ms)")
        
        if "document_updates" in summary:
            update_stats = summary["document_updates"]
            print(f"\nğŸ“ æ–‡æ¡£æ›´æ–°:")
            print(f"  â€¢ æ€»æ›´æ–°æ¬¡æ•°: {update_stats['total_updates']}")
            print(f"  â€¢ æ—¥å‡æ›´æ–°: {update_stats['updates_per_day']:.1f}")
        
        # å»ºè®®
        if analysis["recommendations"]:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for rec in analysis["recommendations"]:
                print(f"  â€¢ {rec}")
        
        if not analysis["recommendations"]:
            print(f"\nâœ… ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–")
    
    elif args.action == "report":
        output_file = args.output or f"performance-report-{datetime.now().strftime('%Y%m%d')}.json"
        report = monitor.generate_report(args.days, output_file)
        print(f"âœ… Performance report generated: {output_file}")
        print(f"ğŸ“Š Health Score: {report['health_score']['score']}/100")
    
    elif args.action == "cleanup":
        monitor.cleanup_old_metrics(args.cleanup_days)
        print(f"âœ… Cleaned up metrics older than {args.cleanup_days} days")
    
    return 0

if __name__ == "__main__":
    exit(main())
